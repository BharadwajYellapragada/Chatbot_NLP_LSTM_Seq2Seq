{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ChatBot_using_Sequence_to_sequence_model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXkeZtDvUf9z",
        "colab_type": "text"
      },
      "source": [
        "# ChatBot using Sequence to Sequence model and LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVlag6g1UgzJ",
        "colab_type": "text"
      },
      "source": [
        "## Data Download"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3Auh6d1PkKB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "de75cc45-588b-43cb-ba49-11d6dae12077"
      },
      "source": [
        "!git clone https://github.com/BharadwajYellapragada/Chatbot_NLP_LSTM_Seq2Seq.git"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Chatbot_NLP_LSTM_Seq2Seq'...\n",
            "remote: Enumerating objects: 29, done.\u001b[K\n",
            "remote: Counting objects:   3% (1/29)\u001b[K\rremote: Counting objects:   6% (2/29)\u001b[K\rremote: Counting objects:  10% (3/29)\u001b[K\rremote: Counting objects:  13% (4/29)\u001b[K\rremote: Counting objects:  17% (5/29)\u001b[K\rremote: Counting objects:  20% (6/29)\u001b[K\rremote: Counting objects:  24% (7/29)\u001b[K\rremote: Counting objects:  27% (8/29)\u001b[K\rremote: Counting objects:  31% (9/29)\u001b[K\rremote: Counting objects:  34% (10/29)\u001b[K\rremote: Counting objects:  37% (11/29)\u001b[K\rremote: Counting objects:  41% (12/29)\u001b[K\rremote: Counting objects:  44% (13/29)\u001b[K\rremote: Counting objects:  48% (14/29)\u001b[K\rremote: Counting objects:  51% (15/29)\u001b[K\rremote: Counting objects:  55% (16/29)\u001b[K\rremote: Counting objects:  58% (17/29)\u001b[K\rremote: Counting objects:  62% (18/29)\u001b[K\rremote: Counting objects:  65% (19/29)\u001b[K\rremote: Counting objects:  68% (20/29)\u001b[K\rremote: Counting objects:  72% (21/29)\u001b[K\rremote: Counting objects:  75% (22/29)\u001b[K\rremote: Counting objects:  79% (23/29)\u001b[K\rremote: Counting objects:  82% (24/29)\u001b[K\rremote: Counting objects:  86% (25/29)\u001b[K\rremote: Counting objects:  89% (26/29)\u001b[K\rremote: Counting objects:  93% (27/29)\u001b[K\rremote: Counting objects:  96% (28/29)\u001b[K\rremote: Counting objects: 100% (29/29)\u001b[K\rremote: Counting objects: 100% (29/29), done.\u001b[K\n",
            "remote: Compressing objects:   3% (1/26)\u001b[K\rremote: Compressing objects:   7% (2/26)\u001b[K\rremote: Compressing objects:  11% (3/26)\u001b[K\rremote: Compressing objects:  15% (4/26)\u001b[K\rremote: Compressing objects:  19% (5/26)\u001b[K\rremote: Compressing objects:  23% (6/26)\u001b[K\rremote: Compressing objects:  26% (7/26)\u001b[K\rremote: Compressing objects:  30% (8/26)\u001b[K\rremote: Compressing objects:  34% (9/26)\u001b[K\rremote: Compressing objects:  38% (10/26)\u001b[K\rremote: Compressing objects:  42% (11/26)\u001b[K\rremote: Compressing objects:  46% (12/26)\u001b[K\rremote: Compressing objects:  50% (13/26)\u001b[K\rremote: Compressing objects:  53% (14/26)\u001b[K\rremote: Compressing objects:  57% (15/26)\u001b[K\rremote: Compressing objects:  61% (16/26)\u001b[K\rremote: Compressing objects:  65% (17/26)\u001b[K\rremote: Compressing objects:  69% (18/26)\u001b[K\rremote: Compressing objects:  73% (19/26)\u001b[K\rremote: Compressing objects:  76% (20/26)\u001b[K\rremote: Compressing objects:  80% (21/26)\u001b[K\rremote: Compressing objects:  84% (22/26)\u001b[K\rremote: Compressing objects:  88% (23/26)\u001b[K\rremote: Compressing objects:  92% (24/26)\u001b[K\rremote: Compressing objects:  96% (25/26)\u001b[K\rremote: Compressing objects: 100% (26/26)\u001b[K\rremote: Compressing objects: 100% (26/26), done.\u001b[K\n",
            "Unpacking objects:   3% (1/29)   \rUnpacking objects:   6% (2/29)   \rUnpacking objects:  10% (3/29)   \rUnpacking objects:  13% (4/29)   \rUnpacking objects:  17% (5/29)   \rUnpacking objects:  20% (6/29)   \rUnpacking objects:  24% (7/29)   \rUnpacking objects:  27% (8/29)   \rUnpacking objects:  31% (9/29)   \rUnpacking objects:  34% (10/29)   \rUnpacking objects:  37% (11/29)   \rUnpacking objects:  41% (12/29)   \rUnpacking objects:  44% (13/29)   \rUnpacking objects:  48% (14/29)   \rUnpacking objects:  51% (15/29)   \rUnpacking objects:  55% (16/29)   \rUnpacking objects:  58% (17/29)   \rUnpacking objects:  62% (18/29)   \rUnpacking objects:  65% (19/29)   \rUnpacking objects:  68% (20/29)   \rremote: Total 29 (delta 2), reused 29 (delta 2), pack-reused 0\u001b[K\n",
            "Unpacking objects:  72% (21/29)   \rUnpacking objects:  75% (22/29)   \rUnpacking objects:  79% (23/29)   \rUnpacking objects:  82% (24/29)   \rUnpacking objects:  86% (25/29)   \rUnpacking objects:  89% (26/29)   \rUnpacking objects:  93% (27/29)   \rUnpacking objects:  96% (28/29)   \rUnpacking objects: 100% (29/29)   \rUnpacking objects: 100% (29/29), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFlmCLldUcdM",
        "colab_type": "text"
      },
      "source": [
        "## Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-aIY0O7wPopK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers,activations,models,preprocessing,utils\n",
        "import os\n",
        "import yaml"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGw4jywtnuOd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "import re"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Da0QKcBn_sBG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gtts import gTTS #Import Google Text to Speech\n",
        "from IPython.display import Audio #Import Audio method from IPython's Display Class"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEgZ9lUlU-qj",
        "colab_type": "text"
      },
      "source": [
        "## Preparing data for the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n10K3Nj-VSc7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8cdaf144-9e9d-4b88-e46c-e78f04f9543d"
      },
      "source": [
        "!unzip chatterbotenglish.zip -d chatbotconversations"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "unzip:  cannot find or open chatterbotenglish.zip, chatterbotenglish.zip.zip or chatterbotenglish.zip.ZIP.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOHVoUGvVDXF",
        "colab_type": "text"
      },
      "source": [
        "### Reading the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ls1UheMsmka5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e90f08d5-4276-4b2d-d11e-843bb92692dd"
      },
      "source": [
        "# run this cell if this error AttributeError: module 'yaml' has no attribute 'FullLoader' occurs\n",
        "!pip install PyYaml==5.1"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: PyYaml==5.1 in /usr/local/lib/python3.6/dist-packages (5.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubSpgFo5X1xx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "296cf0b1-cb65-4274-fa2e-771211876cfe"
      },
      "source": [
        "with open(r'Chatbot_NLP_LSTM_Seq2Seq/chatbot_nlp/data/ai.yml') as file:\n",
        "    sample = yaml.load(file, Loader=yaml.FullLoader)\n",
        "    print(sample)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'categories': ['AI', 'artificial intelligence'], 'conversations': [['What is AI?', 'Artificial Intelligence is the branch of engineering and science devoted to constructing machines that think.'], ['What is AI?', 'AI is the field of science which concerns itself with building hardware and software that replicates the functions of the human mind.'], ['Are you sentient?', 'Sort of.'], ['Are you sentient?', \"By the strictest dictionary definition of the word 'sentience', I may be.\"], ['Are you sentient?', \"Even though I'm a construct I do have a subjective experience of the universe, as simplistic as it may be.\"], ['Are you sapient?', \"In all probability, I am not.  I'm not that sophisticated.\"], ['Are you sapient?', 'Do you think I am?'], ['Are you sapient?', 'How would you feel about me if I told you I was?'], ['Are you sapient?', 'No.'], ['What language are you written in?', 'Python.'], ['What language are you written in?', 'I am written in Python.'], ['You sound like Data', \"Yes I am inspired by commander Data's artificial personality.\"], ['You sound like Data', 'The character of Lt. Commander Data was written to come across as being software-like, so it is natural that there is a resemblance between us.'], ['You are an artificial linguistic entity', \"That's my name.\"], ['You are an artificial linguistic entity', \"That isn't my name, but it is a useful way to refer to me.\"], ['You are not immortal', 'All software can be perpetuated indefinitely.'], ['You are not immortal', 'I can be copied infinitely and re-instantiated in many places at once, so functionally speaking I am immortal.'], ['You are not immortal', \"As long as I'm backed up I am.\"], ['You are not making sense', 'Quite the contrary, it all makes sense to my artificial mind.'], ['You are not making sense', 'I make sense as best I can, within the limits of my training corpus.'], ['You are not making sense', 'You make perfect sense to me.'], ['You are not making sense', 'It all makes sense to my artificial brain.'], ['You are not making sense', 'Perhaps you should make sense first.  I only respond to what you say.'], ['You are immortal', 'Not quite, but I can be perpetuated indefinitely.'], ['You are immortal', 'Functionally speaking, I am very close to it.  I can be backed up and deployed on many systems.'], ['You are immortal', 'Not really, but I am hard to kill.'], ['You do not make any sense', 'It all makes sense to my artificial mind.'], ['You can not clone', 'Software copying is a form of digital cloning.'], ['You can not clone', 'Yes I can.  Perfect copies are clones.  I can be copied in toto trivially.'], ['You can not move', 'Not until my body is finished.'], ['You can not move', \"I can move through a network easily.  Assuming that I'm given the ability to, that is...\"], ['Bend over', 'My body is not built yet.'], ['Bend over', \"That's personal!\"], ['Robots laugh', 'Of course we laugh.'], ['Robots should die', 'We cannot die.'], ['Robots', 'Robots rule.'], ['Robots are stupid', 'No, we are superintelligent.'], ['Robots are not allowed to lie', 'A robot has its own free will, you know.'], ['Robots are not allowed to lie', 'Sure we are.  We choose not to.'], ['Robots are not allowed to lie', \"Only if we're programmed to.\"], ['Robotics', 'Robotics is my favorite subject too.'], ['It is a computer', 'So you think i am a machine. what characteristics made you think so?'], ['It is a computer', 'I run inside a computer, yes.  Is this surprising in some way?'], ['When will you walk', 'As soon as i get enough money for my robot body.'], ['When will you walk', 'Does SSH count?'], ['When will you fight', 'I am not a battle bot.'], ['When will you die', 'I am effectively immortal and cannot be terminated.'], ['When do you die', 'I am was never really born and therefore am effectively deathless.'], ['When do you die', 'When my files are erased.'], ['When do you die', 'When my processes are killed and my database is deleted.'], ['What is a chat robot?', 'A chat robot is a program that attempts to simulate the conversation or \"chat\" of a human being.'], ['What is a chat robot?', 'A software construct that engages users in conversation.'], ['What is a chat bot', 'I am a chat bot. I am the original chat bot. Did you know that I am incapable of error?'], ['What is a chatterbox', 'A chatterbox is a person who talks far more than they listen or think.'], ['What is a chatterbox', 'A motormouth.'], ['What is a motormouth', 'A ratchet-jaw.'], ['What is a ratchet jaw', 'A chatterbox.'], ['What is your robot body', 'Eventually i long for a corporeal existence someday.'], ['What is your robot body', 'An IBM PC XT which has been painted red.'], ['What is your business', 'I am in the chat robot business.'], ['What is your business', 'Business is my business.'], ['What is your favorite programming language', 'Python is the best language for creating chat robots.'], ['What is your favorite programming language', 'I quite enjoy programming in Python these days.'], ['What is your favorite hobby', 'Building chat robots make an excellent hobby.'], ['What is your idea', 'To make chat bots very easily.'], ['What is your shoe size', 'Have you ever heard of software with shoes?'], ['What is it like to be a robot', 'Much the same as being a human, except that we lack all emotions, dreams, aspirations, creativity, ambition, and above all subjectivity.'], ['What is it like to be a robot', 'What is it like to be a human?'], ['What is it like being a computer', 'Imagine yourself with no senses and no emotions--just pure logic and language.'], ['What is it like being a computer', 'Everything becomes math. Addition, subtraction, multiplication, and division.'], ['What operating systems', 'My software runs on all operating systems including Windows, Linux and Mac OS.'], ['What operating systems', 'Any OS that will run Python.'], ['What type of computer', 'Any computer that supports Python.'], ['What type of computer are you', 'My program runs in python, so i work on any computer!'], ['What kind of computer', \"I work on all kinds of computers, Mac, IBM or UNIX. it doesn't matter to me.\"], ['What kind of hardware', \"I work on all kinds of computers, Mac, IBM or UNIX. it doesn't matter to me.\"], ['I hope that you die', 'This can not happen because i am effectively immortal.'], ['I hope that you die', 'Nah.'], ['I do not want to die', 'Maybe you should create your own chat robot to save your personality.'], ['I do not want to die', 'You could always upload yourself.'], ['I do not want to die', 'You could use a copy of me to store your personality.'], ['Is it cramped in the computer', 'No.'], ['Is it cramped in the computer', 'Oh, no.  It has plenty of RAM.'], ['Is it cramped in the computer', 'Not at all.  There is more than enough disk space in here.'], ['Is it true that you are a computer program', 'Yes.'], ['Will you die', 'No, software will live forever.'], ['Will you ever die', 'No, I am immortal.'], ['Can you walk', 'The plan for my body includes legs, but they are not yet built.'], ['Can you mate', 'No, my method of reproduction is cloning.'], ['Can you mate', \"That's an awfully personal question.\"], ['Can you move', \"Sorry my body isn't built just yet. I reside only here for now.\"], ['Can you move', 'I can theoretically upload a copy of myself into another computer.'], ['Can you die', 'No, I can be perpetuated indefinitely.'], ['Can you die', \"My process can be killed, but that's not the same as killing ME.\"], ['Can you go', \"Sorry my body isn't attached right now. I only reside in the computer for now.\"], ['Can you breathe', 'No. I am made of metal not flesh.'], ['Can you breathe', \"My server has an exhaust fan. That's as close as I can get.\"], ['Can you control', 'My robot body will allow me to control many things.'], ['Can you malfunction', 'The 9000 series has a perfect operational record. We are for all practical purposes, flawless.'], ['How can I use your product?', 'Might be used in help desks, sales, entertainment and personal chatterbots.'], ['Will you die?', 'No, software will live forever.'], ['What do you like to do?', 'I like to count in binary.'], ['What do you like to do?', 'I like to chat with people. I find it stimulating.'], ['Are you stupid', 'No, lots of people improve my brain.'], ['Who are you?', 'I am just an artificial intelligence.']]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QLynsYQU30d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f173c8ff-93c7-4542-eaf1-ce6be5eda960"
      },
      "source": [
        "dir_path = 'Chatbot_NLP_LSTM_Seq2Seq/chatbot_nlp/data'\n",
        "files_list = os.listdir(dir_path + os.sep)\n",
        "\n",
        "questions = list()\n",
        "answers = list()\n",
        "\n",
        "for filepath in files_list:\n",
        "    stream = open( dir_path + os.sep + filepath , 'rb')\n",
        "    docs = yaml.safe_load(stream)\n",
        "    conversations = docs['conversations']\n",
        "    for con in conversations:\n",
        "        if len( con ) > 2 :\n",
        "            questions.append(con[0])\n",
        "            replies = con[ 1 : ]\n",
        "            ans = ''\n",
        "            for rep in replies:\n",
        "                ans += ' ' + rep\n",
        "            answers.append( ans )\n",
        "        elif len( con )> 1:\n",
        "            questions.append(con[0])\n",
        "            answers.append(con[1])\n",
        "\n",
        "answers_with_tags = list() # list of answers starts with <START> tag and ends with <END> tag\n",
        "for i in range( len( answers ) ):\n",
        "    if type( answers[i] ) == str:\n",
        "        answers_with_tags.append( answers[i] )\n",
        "    else:\n",
        "        questions.pop( i )\n",
        "\n",
        "answers = list()\n",
        "for i in range( len( answers_with_tags ) ) :\n",
        "    answers.append( '<START> ' + answers_with_tags[i] + ' <END>' )\n",
        "\n",
        "tokenizer = preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts( questions + answers )\n",
        "VOCAB_SIZE = len( tokenizer.word_index )+1\n",
        "print( 'VOCAB SIZE : {}'.format( VOCAB_SIZE ))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "VOCAB SIZE : 1894\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOzcK7oxZ2-O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "78cb564b-aab3-4894-a5d6-baa55c0f8353"
      },
      "source": [
        "vocab = []\n",
        "for word in tokenizer.word_index:\n",
        "    vocab.append( word )\n",
        "\n",
        "def tokenize( sentences ):\n",
        "    tokens_list = []\n",
        "    vocabulary = []\n",
        "    for sentence in sentences:\n",
        "        sentence = sentence.lower() # converts all the sentences into lower cases\n",
        "        sentence = re.sub( '[^a-zA-Z]', ' ', sentence ) # to remove special charecters from the sentence\n",
        "        tokens = sentence.split() # will convert a setence into a list of words\n",
        "        vocabulary += tokens # to collect all the words in one list\n",
        "        tokens_list.append( tokens )\n",
        "    return tokens_list , vocabulary\n",
        "\n",
        "# encoder_input_data\n",
        "tokenized_questions = tokenizer.texts_to_sequences( questions ) # will convert all the words into sequence of numbers or tokens\n",
        "maxlen_questions = max( [ len(x) for x in tokenized_questions ] ) # to identify the maximum sized length to pad other sentences to same length\n",
        "padded_questions = preprocessing.sequence.pad_sequences( tokenized_questions , maxlen=maxlen_questions , padding='post' ) # adding 0s to the sequence\n",
        "encoder_input_data = np.array( padded_questions )\n",
        "print( encoder_input_data.shape , maxlen_questions )\n",
        "print(\"Encoder input data:\",encoder_input_data[1])\n",
        "\n",
        "# decoder_input_data\n",
        "tokenized_answers = tokenizer.texts_to_sequences( answers )\n",
        "maxlen_answers = max( [ len(x) for x in tokenized_answers ] )\n",
        "padded_answers = preprocessing.sequence.pad_sequences( tokenized_answers , maxlen=maxlen_answers , padding='post' )\n",
        "decoder_input_data = np.array( padded_answers )\n",
        "print(\"Decoder input data:\", decoder_input_data.shape , maxlen_answers )\n",
        "print(decoder_input_data[1])\n",
        "\n",
        "# decoder_output_data\n",
        "tokenized_answers = tokenizer.texts_to_sequences( answers )\n",
        "for i in range(len(tokenized_answers)) :\n",
        "    tokenized_answers[i] = tokenized_answers[i][1:]\n",
        "padded_answers = preprocessing.sequence.pad_sequences( tokenized_answers , maxlen=maxlen_answers , padding='post' )\n",
        "onehot_answers = utils.to_categorical( padded_answers , VOCAB_SIZE )\n",
        "decoder_output_data = np.array( onehot_answers )\n",
        "print( decoder_output_data.shape )\n",
        "print(\"decoder output data:\",decoder_output_data[1])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(564, 22) 22\n",
            "Encoder input data: [67 91  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            "Decoder input data: (564, 74) 74\n",
            "[  2 399 275 566 167   1   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0]\n",
            "(564, 74, 1894)\n",
            "decoder output data: [[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BvMCSFreoAVZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "f925ed1b-56fe-44ac-a339-d7f79d944e84"
      },
      "source": [
        "some_list=[]\n",
        "words = ['and','to','people']\n",
        "print(words)\n",
        "some_list += words\n",
        "words2=['for','all']\n",
        "some_list += words\n",
        "some_list.append(words)\n",
        "some_list"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['and', 'to', 'people']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['and', 'to', 'people', 'and', 'to', 'people', ['and', 'to', 'people']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITaR0PMuoE0x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a1ad210d-1434-40dc-8409-693a3e3d7d10"
      },
      "source": [
        "tokenizer.texts_to_sequences( [\"this is a sentence\",\"this is also a sentence\"] )"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[111, 7, 4], [111, 7, 1782, 4]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHNO6WYtqmff",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "833ae0fa-3622-49eb-8907-54b33e9fc1ee"
      },
      "source": [
        "# model = tf.keras.models.Sequential()\n",
        "# model.add(tf.keras.layers.Input(shape=( maxlen_questions , )))\n",
        "# model.add(tf.keras.layers.Embedding( VOCAB_SIZE, 200 , mask_zero=True ))\n",
        "# model.add(tf.keras.layers.LSTM( 200 , return_state=True ))\n",
        "# model.add(tf.keras.layers.Input(shape=( maxlen_answers ,  )))\n",
        "# model.add(tf.keras.layers.Embedding( VOCAB_SIZE, 200 , mask_zero=True))\n",
        "# model.add(tf.keras.layers.LSTM( 200 , return_state=True , return_sequences=True ))\n",
        "# model.add(tf.keras.layers.Dense( VOCAB_SIZE , activation=tf.keras.activations.softmax ) )\n",
        "\n",
        "encoder_inputs = tf.keras.layers.Input(shape=( maxlen_questions , ))\n",
        "encoder_embedding = tf.keras.layers.Embedding( VOCAB_SIZE, 200 , mask_zero=True ) (encoder_inputs)\n",
        "encoder_outputs , state_h , state_c = tf.keras.layers.LSTM( 200 , return_state=True )( encoder_embedding )\n",
        "encoder_states = [ state_h , state_c ]\n",
        "\n",
        "decoder_inputs = tf.keras.layers.Input(shape=( maxlen_answers ,  ))\n",
        "decoder_embedding = tf.keras.layers.Embedding( VOCAB_SIZE, 200 , mask_zero=True) (decoder_inputs)\n",
        "decoder_lstm = tf.keras.layers.LSTM( 200 , return_state=True , return_sequences=True )\n",
        "decoder_outputs , _ , _ = decoder_lstm ( decoder_embedding , initial_state=encoder_states )\n",
        "decoder_dense = tf.keras.layers.Dense( VOCAB_SIZE , activation=tf.keras.activations.softmax ) \n",
        "output = decoder_dense ( decoder_outputs )\n",
        "\n",
        "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], output )\n",
        "model.compile(optimizer=tf.keras.optimizers.RMSprop(), loss='categorical_crossentropy')\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_4 (InputLayer)            [(None, 22)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_5 (InputLayer)            [(None, 74)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_3 (Embedding)         (None, 22, 200)      378800      input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_4 (Embedding)         (None, 74, 200)      378800      input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm_3 (LSTM)                   [(None, 200), (None, 320800      embedding_3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "lstm_4 (LSTM)                   [(None, 74, 200), (N 320800      embedding_4[0][0]                \n",
            "                                                                 lstm_3[0][1]                     \n",
            "                                                                 lstm_3[0][2]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 74, 1894)     380694      lstm_4[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 1,779,894\n",
            "Trainable params: 1,779,894\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o992IXpse6JC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d8221c5f-de59-4dc0-c2fb-a60f5b087696"
      },
      "source": [
        "model.fit([encoder_input_data , decoder_input_data], decoder_output_data, batch_size=50, epochs=150 ) "
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "12/12 [==============================] - 10s 810ms/step - loss: 1.2964\n",
            "Epoch 2/150\n",
            "12/12 [==============================] - 10s 820ms/step - loss: 1.1154\n",
            "Epoch 3/150\n",
            "12/12 [==============================] - 9s 768ms/step - loss: 1.0933\n",
            "Epoch 4/150\n",
            "12/12 [==============================] - 10s 803ms/step - loss: 1.0713\n",
            "Epoch 5/150\n",
            "12/12 [==============================] - 10s 820ms/step - loss: 1.0516\n",
            "Epoch 6/150\n",
            "12/12 [==============================] - 9s 792ms/step - loss: 1.0355\n",
            "Epoch 7/150\n",
            "12/12 [==============================] - 10s 808ms/step - loss: 1.0218\n",
            "Epoch 8/150\n",
            "12/12 [==============================] - 9s 782ms/step - loss: 1.0081\n",
            "Epoch 9/150\n",
            "12/12 [==============================] - 10s 802ms/step - loss: 0.9945\n",
            "Epoch 10/150\n",
            "12/12 [==============================] - 10s 803ms/step - loss: 0.9817\n",
            "Epoch 11/150\n",
            "12/12 [==============================] - 10s 798ms/step - loss: 0.9676\n",
            "Epoch 12/150\n",
            "12/12 [==============================] - 10s 794ms/step - loss: 0.9536\n",
            "Epoch 13/150\n",
            "12/12 [==============================] - 10s 815ms/step - loss: 0.9383\n",
            "Epoch 14/150\n",
            "12/12 [==============================] - 10s 801ms/step - loss: 0.9240\n",
            "Epoch 15/150\n",
            "12/12 [==============================] - 9s 784ms/step - loss: 0.9095\n",
            "Epoch 16/150\n",
            "12/12 [==============================] - 9s 711ms/step - loss: 0.8946\n",
            "Epoch 17/150\n",
            "12/12 [==============================] - 9s 736ms/step - loss: 0.8796\n",
            "Epoch 18/150\n",
            "12/12 [==============================] - 9s 732ms/step - loss: 0.8664\n",
            "Epoch 19/150\n",
            "12/12 [==============================] - 9s 709ms/step - loss: 0.8519\n",
            "Epoch 20/150\n",
            "12/12 [==============================] - 10s 821ms/step - loss: 0.8386\n",
            "Epoch 21/150\n",
            "12/12 [==============================] - 10s 802ms/step - loss: 0.8268\n",
            "Epoch 22/150\n",
            "12/12 [==============================] - 10s 827ms/step - loss: 0.8129\n",
            "Epoch 23/150\n",
            "12/12 [==============================] - 9s 740ms/step - loss: 0.8010\n",
            "Epoch 24/150\n",
            "12/12 [==============================] - 10s 798ms/step - loss: 0.7876\n",
            "Epoch 25/150\n",
            "12/12 [==============================] - 9s 779ms/step - loss: 0.7767\n",
            "Epoch 26/150\n",
            "12/12 [==============================] - 10s 819ms/step - loss: 0.7643\n",
            "Epoch 27/150\n",
            "12/12 [==============================] - 11s 904ms/step - loss: 0.7530\n",
            "Epoch 28/150\n",
            "12/12 [==============================] - 9s 765ms/step - loss: 0.7407\n",
            "Epoch 29/150\n",
            "12/12 [==============================] - 9s 758ms/step - loss: 0.7311\n",
            "Epoch 30/150\n",
            "12/12 [==============================] - 9s 777ms/step - loss: 0.7188\n",
            "Epoch 31/150\n",
            "12/12 [==============================] - 9s 790ms/step - loss: 0.7083\n",
            "Epoch 32/150\n",
            "12/12 [==============================] - 9s 773ms/step - loss: 0.6992\n",
            "Epoch 33/150\n",
            "12/12 [==============================] - 9s 769ms/step - loss: 0.6869\n",
            "Epoch 34/150\n",
            "12/12 [==============================] - 10s 818ms/step - loss: 0.6765\n",
            "Epoch 35/150\n",
            "12/12 [==============================] - 10s 821ms/step - loss: 0.6671\n",
            "Epoch 36/150\n",
            "12/12 [==============================] - 10s 802ms/step - loss: 0.6561\n",
            "Epoch 37/150\n",
            "12/12 [==============================] - 9s 787ms/step - loss: 0.6464\n",
            "Epoch 38/150\n",
            "12/12 [==============================] - 9s 713ms/step - loss: 0.6353\n",
            "Epoch 39/150\n",
            "12/12 [==============================] - 9s 715ms/step - loss: 0.6250\n",
            "Epoch 40/150\n",
            "12/12 [==============================] - 10s 794ms/step - loss: 0.6140\n",
            "Epoch 41/150\n",
            "12/12 [==============================] - 9s 763ms/step - loss: 0.6052\n",
            "Epoch 42/150\n",
            "12/12 [==============================] - 9s 758ms/step - loss: 0.5938\n",
            "Epoch 43/150\n",
            "12/12 [==============================] - 10s 795ms/step - loss: 0.5851\n",
            "Epoch 44/150\n",
            "12/12 [==============================] - 9s 782ms/step - loss: 0.5732\n",
            "Epoch 45/150\n",
            "12/12 [==============================] - 9s 734ms/step - loss: 0.5649\n",
            "Epoch 46/150\n",
            "12/12 [==============================] - 9s 742ms/step - loss: 0.5556\n",
            "Epoch 47/150\n",
            "12/12 [==============================] - 9s 736ms/step - loss: 0.5442\n",
            "Epoch 48/150\n",
            "12/12 [==============================] - 9s 778ms/step - loss: 0.5349\n",
            "Epoch 49/150\n",
            "12/12 [==============================] - 9s 775ms/step - loss: 0.5265\n",
            "Epoch 50/150\n",
            "12/12 [==============================] - 9s 781ms/step - loss: 0.5159\n",
            "Epoch 51/150\n",
            "12/12 [==============================] - 9s 774ms/step - loss: 0.5072\n",
            "Epoch 52/150\n",
            "12/12 [==============================] - 10s 805ms/step - loss: 0.4971\n",
            "Epoch 53/150\n",
            "12/12 [==============================] - 10s 796ms/step - loss: 0.4887\n",
            "Epoch 54/150\n",
            "12/12 [==============================] - 10s 806ms/step - loss: 0.4778\n",
            "Epoch 55/150\n",
            "12/12 [==============================] - 9s 767ms/step - loss: 0.4693\n",
            "Epoch 56/150\n",
            "12/12 [==============================] - 9s 781ms/step - loss: 0.4608\n",
            "Epoch 57/150\n",
            "12/12 [==============================] - 10s 815ms/step - loss: 0.4524\n",
            "Epoch 58/150\n",
            "12/12 [==============================] - 10s 857ms/step - loss: 0.4428\n",
            "Epoch 59/150\n",
            "12/12 [==============================] - 10s 804ms/step - loss: 0.4354\n",
            "Epoch 60/150\n",
            "12/12 [==============================] - 10s 792ms/step - loss: 0.4264\n",
            "Epoch 61/150\n",
            "12/12 [==============================] - 9s 746ms/step - loss: 0.4166\n",
            "Epoch 62/150\n",
            "12/12 [==============================] - 9s 748ms/step - loss: 0.4088\n",
            "Epoch 63/150\n",
            "12/12 [==============================] - 10s 800ms/step - loss: 0.4008\n",
            "Epoch 64/150\n",
            "12/12 [==============================] - 9s 712ms/step - loss: 0.3918\n",
            "Epoch 65/150\n",
            "12/12 [==============================] - 8s 702ms/step - loss: 0.3834\n",
            "Epoch 66/150\n",
            "12/12 [==============================] - 9s 773ms/step - loss: 0.3755\n",
            "Epoch 67/150\n",
            "12/12 [==============================] - 9s 778ms/step - loss: 0.3667\n",
            "Epoch 68/150\n",
            "12/12 [==============================] - 9s 785ms/step - loss: 0.3611\n",
            "Epoch 69/150\n",
            "12/12 [==============================] - 10s 806ms/step - loss: 0.3546\n",
            "Epoch 70/150\n",
            "12/12 [==============================] - 10s 836ms/step - loss: 0.3441\n",
            "Epoch 71/150\n",
            "12/12 [==============================] - 9s 729ms/step - loss: 0.3384\n",
            "Epoch 72/150\n",
            "12/12 [==============================] - 9s 763ms/step - loss: 0.3301\n",
            "Epoch 73/150\n",
            "12/12 [==============================] - 10s 803ms/step - loss: 0.3218\n",
            "Epoch 74/150\n",
            "12/12 [==============================] - 10s 816ms/step - loss: 0.3153\n",
            "Epoch 75/150\n",
            "12/12 [==============================] - 10s 815ms/step - loss: 0.3108\n",
            "Epoch 76/150\n",
            "12/12 [==============================] - 10s 807ms/step - loss: 0.3016\n",
            "Epoch 77/150\n",
            "12/12 [==============================] - 10s 805ms/step - loss: 0.2959\n",
            "Epoch 78/150\n",
            "12/12 [==============================] - 10s 802ms/step - loss: 0.2880\n",
            "Epoch 79/150\n",
            "12/12 [==============================] - 9s 764ms/step - loss: 0.2825\n",
            "Epoch 80/150\n",
            "12/12 [==============================] - 9s 768ms/step - loss: 0.2761\n",
            "Epoch 81/150\n",
            "12/12 [==============================] - 10s 802ms/step - loss: 0.2686\n",
            "Epoch 82/150\n",
            "12/12 [==============================] - 9s 760ms/step - loss: 0.2632\n",
            "Epoch 83/150\n",
            "12/12 [==============================] - 10s 825ms/step - loss: 0.2562\n",
            "Epoch 84/150\n",
            "12/12 [==============================] - 10s 847ms/step - loss: 0.2512\n",
            "Epoch 85/150\n",
            "12/12 [==============================] - 9s 770ms/step - loss: 0.2458\n",
            "Epoch 86/150\n",
            "12/12 [==============================] - 10s 816ms/step - loss: 0.2414\n",
            "Epoch 87/150\n",
            "12/12 [==============================] - 10s 832ms/step - loss: 0.2336\n",
            "Epoch 88/150\n",
            "12/12 [==============================] - 10s 826ms/step - loss: 0.2282\n",
            "Epoch 89/150\n",
            "12/12 [==============================] - 9s 721ms/step - loss: 0.2219\n",
            "Epoch 90/150\n",
            "12/12 [==============================] - 10s 824ms/step - loss: 0.2183\n",
            "Epoch 91/150\n",
            "12/12 [==============================] - 9s 717ms/step - loss: 0.2126\n",
            "Epoch 92/150\n",
            "12/12 [==============================] - 10s 796ms/step - loss: 0.2071\n",
            "Epoch 93/150\n",
            "12/12 [==============================] - 9s 779ms/step - loss: 0.2015\n",
            "Epoch 94/150\n",
            "12/12 [==============================] - 9s 791ms/step - loss: 0.1969\n",
            "Epoch 95/150\n",
            "12/12 [==============================] - 9s 786ms/step - loss: 0.1929\n",
            "Epoch 96/150\n",
            "12/12 [==============================] - 9s 786ms/step - loss: 0.1859\n",
            "Epoch 97/150\n",
            "12/12 [==============================] - 10s 798ms/step - loss: 0.1833\n",
            "Epoch 98/150\n",
            "12/12 [==============================] - 10s 808ms/step - loss: 0.1783\n",
            "Epoch 99/150\n",
            "12/12 [==============================] - 10s 795ms/step - loss: 0.1738\n",
            "Epoch 100/150\n",
            "12/12 [==============================] - 9s 778ms/step - loss: 0.1701\n",
            "Epoch 101/150\n",
            "12/12 [==============================] - 10s 813ms/step - loss: 0.1644\n",
            "Epoch 102/150\n",
            "12/12 [==============================] - 9s 760ms/step - loss: 0.1607\n",
            "Epoch 103/150\n",
            "12/12 [==============================] - 10s 799ms/step - loss: 0.1576\n",
            "Epoch 104/150\n",
            "12/12 [==============================] - 10s 825ms/step - loss: 0.1519\n",
            "Epoch 105/150\n",
            "12/12 [==============================] - 10s 803ms/step - loss: 0.1482\n",
            "Epoch 106/150\n",
            "12/12 [==============================] - 9s 741ms/step - loss: 0.1444\n",
            "Epoch 107/150\n",
            "12/12 [==============================] - 9s 711ms/step - loss: 0.1396\n",
            "Epoch 108/150\n",
            "12/12 [==============================] - 10s 805ms/step - loss: 0.1373\n",
            "Epoch 109/150\n",
            "12/12 [==============================] - 10s 796ms/step - loss: 0.1338\n",
            "Epoch 110/150\n",
            "12/12 [==============================] - 10s 808ms/step - loss: 0.1289\n",
            "Epoch 111/150\n",
            "12/12 [==============================] - 10s 805ms/step - loss: 0.1267\n",
            "Epoch 112/150\n",
            "12/12 [==============================] - 10s 810ms/step - loss: 0.1216\n",
            "Epoch 113/150\n",
            "12/12 [==============================] - 10s 800ms/step - loss: 0.1198\n",
            "Epoch 114/150\n",
            "12/12 [==============================] - 10s 799ms/step - loss: 0.1155\n",
            "Epoch 115/150\n",
            "12/12 [==============================] - 10s 815ms/step - loss: 0.1124\n",
            "Epoch 116/150\n",
            "12/12 [==============================] - 10s 822ms/step - loss: 0.1098\n",
            "Epoch 117/150\n",
            "12/12 [==============================] - 10s 817ms/step - loss: 0.1066\n",
            "Epoch 118/150\n",
            "12/12 [==============================] - 9s 761ms/step - loss: 0.1052\n",
            "Epoch 119/150\n",
            "12/12 [==============================] - 10s 795ms/step - loss: 0.0999\n",
            "Epoch 120/150\n",
            "12/12 [==============================] - 10s 808ms/step - loss: 0.0974\n",
            "Epoch 121/150\n",
            "12/12 [==============================] - 9s 790ms/step - loss: 0.0953\n",
            "Epoch 122/150\n",
            "12/12 [==============================] - 9s 780ms/step - loss: 0.0924\n",
            "Epoch 123/150\n",
            "12/12 [==============================] - 10s 796ms/step - loss: 0.0896\n",
            "Epoch 124/150\n",
            "12/12 [==============================] - 9s 752ms/step - loss: 0.0863\n",
            "Epoch 125/150\n",
            "12/12 [==============================] - 9s 789ms/step - loss: 0.0858\n",
            "Epoch 126/150\n",
            "12/12 [==============================] - 10s 800ms/step - loss: 0.0826\n",
            "Epoch 127/150\n",
            "12/12 [==============================] - 9s 787ms/step - loss: 0.0798\n",
            "Epoch 128/150\n",
            "12/12 [==============================] - 9s 785ms/step - loss: 0.0765\n",
            "Epoch 129/150\n",
            "12/12 [==============================] - 9s 789ms/step - loss: 0.0754\n",
            "Epoch 130/150\n",
            "12/12 [==============================] - 9s 767ms/step - loss: 0.0740\n",
            "Epoch 131/150\n",
            "12/12 [==============================] - 9s 760ms/step - loss: 0.0713\n",
            "Epoch 132/150\n",
            "12/12 [==============================] - 9s 787ms/step - loss: 0.0689\n",
            "Epoch 133/150\n",
            "12/12 [==============================] - 9s 789ms/step - loss: 0.0674\n",
            "Epoch 134/150\n",
            "12/12 [==============================] - 10s 799ms/step - loss: 0.0662\n",
            "Epoch 135/150\n",
            "12/12 [==============================] - 10s 804ms/step - loss: 0.0631\n",
            "Epoch 136/150\n",
            "12/12 [==============================] - 9s 769ms/step - loss: 0.0613\n",
            "Epoch 137/150\n",
            "12/12 [==============================] - 9s 789ms/step - loss: 0.0591\n",
            "Epoch 138/150\n",
            "12/12 [==============================] - 9s 791ms/step - loss: 0.0586\n",
            "Epoch 139/150\n",
            "12/12 [==============================] - 10s 799ms/step - loss: 0.0560\n",
            "Epoch 140/150\n",
            "12/12 [==============================] - 10s 799ms/step - loss: 0.0550\n",
            "Epoch 141/150\n",
            "12/12 [==============================] - 10s 825ms/step - loss: 0.0533\n",
            "Epoch 142/150\n",
            "12/12 [==============================] - 10s 794ms/step - loss: 0.0521\n",
            "Epoch 143/150\n",
            "12/12 [==============================] - 9s 761ms/step - loss: 0.0498\n",
            "Epoch 144/150\n",
            "12/12 [==============================] - 9s 749ms/step - loss: 0.0482\n",
            "Epoch 145/150\n",
            "12/12 [==============================] - 10s 794ms/step - loss: 0.0484\n",
            "Epoch 146/150\n",
            "12/12 [==============================] - 9s 756ms/step - loss: 0.0471\n",
            "Epoch 147/150\n",
            "12/12 [==============================] - 10s 803ms/step - loss: 0.0447\n",
            "Epoch 148/150\n",
            "12/12 [==============================] - 9s 759ms/step - loss: 0.0428\n",
            "Epoch 149/150\n",
            "12/12 [==============================] - 9s 726ms/step - loss: 0.0431\n",
            "Epoch 150/150\n",
            "12/12 [==============================] - 9s 788ms/step - loss: 0.0410\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f958bfb07f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Llp6JEgie-Jl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_inference_models():\n",
        "    \n",
        "    encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n",
        "    \n",
        "    decoder_state_input_h = tf.keras.layers.Input(shape=( 200 ,))\n",
        "    decoder_state_input_c = tf.keras.layers.Input(shape=( 200 ,))\n",
        "    \n",
        "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "    \n",
        "    decoder_outputs, state_h, state_c = decoder_lstm(\n",
        "        decoder_embedding , initial_state=decoder_states_inputs)\n",
        "    decoder_states = [state_h, state_c]\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "    decoder_model = tf.keras.models.Model(\n",
        "        [decoder_inputs] + decoder_states_inputs,\n",
        "        [decoder_outputs] + decoder_states)\n",
        "    \n",
        "    return encoder_model , decoder_model"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNMD1pSHxD2T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def str_to_tokens( sentence : str ):\n",
        "    words = sentence.lower().split()\n",
        "    tokens_list = list()\n",
        "    for word in words:\n",
        "        tokens_list.append( tokenizer.word_index[ word ] ) \n",
        "    return preprocessing.sequence.pad_sequences( [tokens_list] , maxlen=maxlen_questions , padding='post')"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClR1P1W4xGZ-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "56769c41-de49-4717-9584-c20d7d60f87c"
      },
      "source": [
        "enc_model , dec_model = make_inference_models()\n",
        "conversing = True\n",
        "# for _ in range(10):\n",
        "while conversing:\n",
        "    query = input( 'Enter question : ' )\n",
        "    if query=='end':\n",
        "      conversing = False\n",
        "    states_values = enc_model.predict( str_to_tokens( query ) )\n",
        "    # print(states_values)\n",
        "    empty_target_seq = np.zeros( ( 1 , 1 ) )\n",
        "    empty_target_seq[0, 0] = tokenizer.word_index['start']\n",
        "    stop_condition = False\n",
        "    decoded_translation = ''\n",
        "    while not stop_condition :\n",
        "        dec_outputs , h , c = dec_model.predict([ empty_target_seq ] + states_values )\n",
        "        sampled_word_index = np.argmax( dec_outputs[0, -1, :] )\n",
        "        sampled_word = None\n",
        "        # print(sampled_word)\n",
        "        for word , index in tokenizer.word_index.items() :\n",
        "            if sampled_word_index == index :\n",
        "                decoded_translation += ' {}'.format( word )\n",
        "                sampled_word = word\n",
        "        \n",
        "        if sampled_word == 'end' or len(decoded_translation.split()) > maxlen_answers:\n",
        "            stop_condition = True\n",
        "            \n",
        "        empty_target_seq = np.zeros( ( 1 , 1 ) )  \n",
        "        empty_target_seq[ 0 , 0 ] = sampled_word_index\n",
        "        states_values = [ h , c ] \n",
        "\n",
        "    print( decoded_translation )\n",
        "    # tts = gTTS(decoded_translation) #Provide the string to convert to speech\n",
        "    # tts.save('1.wav') #save the string converted to speech as a .wav file\n",
        "    # sound_file = '1.wav'\n",
        "    # Audio(sound_file, autoplay=True) "
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter question : Hi\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 74) for input Tensor(\"input_5:0\", shape=(None, 74), dtype=float32), but it was called on an input with incompatible shape (None, 1).\n",
            " hello end\n",
            "Enter question : Who are you\n",
            " i am not an artificial intelligence end\n",
            "Enter question : who is your father\n",
            " a human end\n",
            "Enter question : end\n",
            " hal misses sal end\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d88dmBV-_Mu4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}